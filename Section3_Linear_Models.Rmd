---
output: github_document
---

# Linear Models
In this file, we will build linear models to predict Log(AV_TOTAL).

```{r, echo=FALSE}
library(tidyverse)
library(glmnet)
library(leaps)
library(modelr)

options("scipen"=100, "digits"=4)

boston <- read.csv("Data/Boston_Taxes_working_data.csv")
```

In our data, there are a few extraneous columns that we will not be using in this script.  There is the raw response variable, AV_TOTAL, and a scaled version of the response, av_total_scaled.  There is also an unique identifier for each row, PID, which we will not need for the time being.
```{r}
pred <- c("GROSS_AREA",                  "U_KITCH_STYLE_Luxury",        "U_INTERIOR_FINISH_fi",       
          "NUM_FLOORS",                  "U_FPLACE",                    "U_VIEW_Excellent",           
          "U_VIEW_Special",              "U_FULL_BTH",                  "YR_BUILT",                   
          "U_INT_CND_Excellent",         "U_BDRMS",                     "U_HALF_BTH",                 
          "U_ORIENT_fixed_Throu",        "U_NUM_PARK",                  "U_HEAT_TYPE_fixed_Fo",       
          "U_KITCH_TYPE_Full_ea",        "U_BTH_STYLE_Luxury",          "U_HEAT_TYPE_fixed_Ho",       
          "U_BASE_FLOOR",                "U_VIEW_Good",                 "U_CORNER_Yes",               
          "U_ORIENT_fixed_Face_",        "U_TOT_RMS",                   "U_ORIENT_fixed_Middl",       
          "U_INTERIOR_FINISH_fi2",       "U_ORIENT_fixed_Rear_",        "U_INT_CND_Good",             
          "U_VIEW_Fair",                 "U_KITCH_STYLE_No_remodeling", "U_KITCH_STYLE_Semi_modern",  
          "U_ORIENT_fixed_End",          "U_ORIENT_fixed_Rear_above",   "U_ORIENT_fixed_Unknown",     
          "U_HEAT_TYPE_fixed_Heat_Pump", "U_KITCH_TYPE_None",           "U_BTH_STYLE_No_remodeling",  
          "U_BTH_STYLE_Semi_modern",     "Log_LIVING_AREA_"         
)

boston_p <- select(boston, c("Log_AV_TOTAL_", pred))
```

## Full Model

We will start by building a model to predict Log_AV_TOTAL that ignores the ward of the condo.  

The first task is to build a linear regression model with all the predictors.  This should give us a sense of what predictors are important.  It also gives us an opportunity to look at the variance of the model to check for non-constant variance in the errors.
```{r}
boston.lm <- lm(Log_AV_TOTAL_ ~ ., data = boston_p)
summary(boston.lm)
```

Plot our regression model.  We see some deviation from normality in the residuals in the Q-Q plot.  I think that deviation is probably acceptable.  There does appear to be non-constant variance in the residuals in the Residuals vs Fitted plot.
```{r}
par(mfrow=c(2,2))
plot(boston.lm)
```

The mean squared error of the model is 0.1149.
```{r}
mean((predict(boston.lm) - boston$Log_AV_TOTAL_)^2)
```


We built our full model and found the mean squared error.  Now, we want to take a closer look at the predictions and examine the values that the model did not predict well.
```{r}
predictions <- boston %>% 
  add_predictions(boston.lm, var = "full_pred") %>%
  add_residuals(boston.lm, var = "full_resid") %>%
  mutate(mse  = (full_pred - Log_AV_TOTAL_)^2) %>% 
  select(PID, Ward, Log_AV_TOTAL_, full_pred, full_resid, mse) 
```

Here we look at the mean squared error for each of the wards.  This gives us a sense of which wards the full model does a good job predicting (10, 11 21, 22) and which wards it does a bad job of prediciting (12, 14, 18). 
```{r}
predictions %>% 
  group_by(Ward) %>% 
  summarize(mean_mse = mean(mse)) %>% 
  print(n=1e3)
```



###Lasso model selection 

Now we will start looking at variable selection methods to simplify the model.  The first method will be to use Lasso to eliminate some of the extraneous variables.

We are able to completely remove U_HEAT_TYPE_fixed_Ho and several predictors are very close to zero.  This model improved our mse slightly to 0.1138.
```{r}
x <- model.matrix(Log_AV_TOTAL_ ~ ., boston_p)[,-1]
y <- boston_p$Log_AV_TOTAL_

set.seed(1)
train <-  sample(1:nrow(boston_p), nrow(boston_p)*0.8)
test <- (-train)

lasso.boston <- glmnet(x[train,], y[train], alpha = 1)
cv.boston <- cv.glmnet(x[train,], y[train], alpha=1)
  
bestlam <- cv.boston$lambda.min
lasso.pred <- predict(lasso.boston, s=bestlam, newx=x[test,])
mse <- mean((lasso.pred - y[test])^2)

final.boston <- glmnet(x, y, alpha = 1)
lasso.coef <-  predict(final.boston, type="coefficients", s = bestlam)

print(mse)
print(lasso.coef)
```

```{r}

lp <- predict(final.boston, s=bestlam, newx=x)
lr <- boston$Log_AV_TOTAL_ - lp
lasso <- as_tibble(cbind(boston$PID, round(lp, digits = 3), round(lr, digits = 3)))
names(lasso) <- c("PID","lasso_pred", "lasso_resid")


```


### Ward Analysis with Lasso

## Ward 14

Create a new data frame that only contains the observatiosn from Ward 14 and create the matrices needed to perform Lasso
```{r}
ward_14 <- select(boston, c("Log_AV_TOTAL_", "Ward", pred)) %>% 
  filter(Ward == 14)

x <- model.matrix(Log_AV_TOTAL_ ~ ., ward_14[,names(ward_14) != "Ward"])[,-1]
y <- ward_14$Log_AV_TOTAL_
```

Create a train / test split for the data
```{r}
set.seed(1)
train <-  sample(1:nrow(ward_14), nrow(ward_14)*0.8)
test <- (-train)
```

Perform lasso on test data and plot result.
```{r}
lasso.ward14 <- glmnet(x[train,], y[train], alpha = 1)
plot(lasso.ward14, label = TRUE)
```



Perform cross validation to find the best lambda value.
```{r}
cv.ward14 <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.ward14)
```

Find the best lambda value and make predictios on the test set.  Find the mean squared error on the test set.
```{r}
bestlam <- cv.ward14$lambda.min
print(bestlam)
lasso.pred <- predict(lasso.ward14, s=bestlam, newx=x[test,])
print(mean((lasso.pred - y[test])^2))
```

Use the smallest lambda that is within one standard deviation of best lambda.  Find the mean squared error on the test set.
```{r}
lam_1se <- cv.ward14$lambda.1se
print(lam_1se)
lasso.pred <- predict(lasso.ward14, s=lam_1se, newx=x[test,])
print(mean((lasso.pred - y[test])^2))
```


Using the best lambda value, create a final model using all the data for Ward 14.  Print the model coefficients.
```{r}
final <- glmnet(x, y, alpha = 1)
lasso.coef <-  predict(final, type="coefficients", s = bestlam)
lasso.coef
```

Using the one standard error lambda value, create a final model using all the data for Ward 14.  Print the model coefficients.
```{r}
final <- glmnet(x, y, alpha = 1)
lasso.coef <-  predict(final, type="coefficients", s = lam_1se)
lasso.coef
```


## Ward Analysis Loop

Create a data frame to hold my predictor coefficients
```{r}
ward_coef <- as_tibble(c("Intercept", pred))
names(ward_coef) <- c("Predictor")
```



```{r}
wards = 22
variables = 39
output_best <- matrix(ncol=wards, nrow=variables)
output_1se <- matrix(ncol=wards, nrow=variables)

mse_bestlam <- rep(0,22) 
mse_1se <- rep(0,22) 
 
```

```{r}
for(i in 1:22) {
  ward <- select(boston, c("Log_AV_TOTAL_", "Ward", pred)) %>% 
    filter(Ward == i)

  x <- model.matrix(Log_AV_TOTAL_ ~ ., ward[,names(ward) != "Ward"])[,-1]
  y <- ward$Log_AV_TOTAL_

  set.seed(1)
  train <-  sample(1:nrow(ward), nrow(ward)*0.8)
  test <- (-train)

  lasso.ward <- glmnet(x[train,], y[train], alpha = 1)
  cv.ward <- cv.glmnet(x[train,], y[train], alpha=1)
  
  # use best lambda
  bestlam <- cv.ward$lambda.min
  lasso.pred <- predict(lasso.ward, s=bestlam, newx=x[test,])
  mse_bestlam[i] <- mean((lasso.pred - y[test])^2)

  # use one se lambda
  lam_1se <- cv.ward$lambda.1se
  lasso.pred <- predict(lasso.ward, s=lam_1se, newx=x[test,])
  mse_1se[i] <- mean((lasso.pred - y[test])^2)

  final <- glmnet(x, y, alpha = 1)
  
  lasso.best_coef <-  predict(final, type="coefficients", s = bestlam)
  lasso.1se_coef <-  predict(final, type="coefficients", s = lam_1se)
  
  output_best[,i] <- as.matrix(lasso.best_coef)
  output_1se[,i] <- as.matrix(lasso.1se_coef)
  
}
```

Print the mean squared errors associated with the best lambda values.
```{r}
print(mse_bestlam)
```

Print the mean squared errors associated with the largest lambda that is within one standard deviation of minimum.
```{r}
print(mse_1se)
```



```{r}
p <- c("Intercept",pred)
ward_best <- as_tibble(cbind(p,output_best))
ward_1se <- as_tibble(cbind(p,output_1se))


names(ward_best) = c("Predictor","Ward_1","Ward_2","Ward_3","Ward_4","Ward_5","Ward_6","Ward_7","Ward_8","Ward_9","Ward_10","Ward_11","Ward_12","Ward_13","Ward_14","Ward_15","Ward_16","Ward_17","Ward_18","Ward_19","Ward_20","Ward_21","Ward_22")


names(ward_1se) = c("Predictor","Ward_1","Ward_2","Ward_3","Ward_4","Ward_5","Ward_6","Ward_7","Ward_8","Ward_9","Ward_10","Ward_11","Ward_12","Ward_13","Ward_14","Ward_15","Ward_16","Ward_17","Ward_18","Ward_19","Ward_20","Ward_21","Ward_22")

write_csv(ward_best, "Boston_Taxes_Lasso_Coef_best.csv")
write_csv(ward_1se, "Boston_Taxes_Lasso_Coef_1se.csv")

```













































